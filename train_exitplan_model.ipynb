{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM1AXPAaWUZOs/PQZDq2701",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Imjaeseokk/streamlit-test/blob/main/train_exitplan_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I9Vkuv3lcb1i"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 환경 설정\n",
        "BOARD_SIZE = 7  # 7x7 보드\n",
        "ACTION_DIM = 6 * 4  # 말 개수 * 4 방향\n",
        "STATE_DIM = BOARD_SIZE * BOARD_SIZE  # Flatten된 상태 크기\n",
        "GAMMA = 0.99  # 할인율\n",
        "LR = 0.001  # 학습률\n",
        "BATCH_SIZE = 64  # 미니배치 크기\n",
        "TARGET_UPDATE = 10  # 타겟 네트워크 업데이트 주기\n",
        "MEMORY_CAPACITY = 10000  # 리플레이 메모리 크기\n"
      ],
      "metadata": {
        "id": "nQTlPEuodAPg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DQN 모델 정의\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "z4K6hLItdLWI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 환경 초기화 함수\n",
        "def reset_environment():\n",
        "    global whites, blacks, grid\n",
        "    whites = {1: (0, 1), 2: (1, 2), 3: (2, 1), 4: (4, 1), 5: (5, 2), 6: (6, 1)}\n",
        "    blacks = {1: (0, 5), 2: (1, 4), 3: (2, 5), 4: (4, 5), 5: (5, 4), 6: (6, 5)}\n",
        "    obstacles = [(3, 0), (3, 6)]\n",
        "    goal = (3, 3)\n",
        "\n",
        "    grid = np.zeros((BOARD_SIZE, BOARD_SIZE))\n",
        "    grid[goal] = 9  # 목표\n",
        "    for obtacle in obstacles:\n",
        "        grid[obtacle] = 5\n",
        "    for w, b in zip(whites.values(), blacks.values()):\n",
        "        grid[w] = 1\n",
        "        grid[b] = 2\n",
        "    return grid.flatten()  # Flatten된 상태 반환"
      ],
      "metadata": {
        "id": "od2jeAQudMc_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이동 함수\n",
        "def move(piece, direction, side):\n",
        "    global whites, blacks, grid\n",
        "    directions = {\"left\": (0, -1), \"right\": (0, 1), \"up\": (-1, 0), \"down\": (1, 0)}\n",
        "\n",
        "\n",
        "    if side == \"white\":\n",
        "        selected_coord = whites[piece]\n",
        "    else:\n",
        "        selected_coord = blacks[piece]\n",
        "\n",
        "    grid[selected_coord] = 0\n",
        "    y, x = selected_coord\n",
        "    dy, dx = directions[direction]\n",
        "\n",
        "    # print(\"w:\", whites)\n",
        "    # print(\"b:\", blacks)\n",
        "\n",
        "    while True:\n",
        "        if 0 > y + dy or y + dy >= BOARD_SIZE or 0 > x + dx or x + dx >= BOARD_SIZE:\n",
        "            break\n",
        "        if grid[(y + dy, x + dx)] in [0, 9]:  # 이동 가능\n",
        "            y += dy\n",
        "            x += dx\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    new_coord = (y, x)\n",
        "    if grid[new_coord] == 9:  # 목표 도달\n",
        "        if side == \"white\":\n",
        "            del whites[piece]\n",
        "            print(whites)\n",
        "        else:\n",
        "            del blacks[piece]\n",
        "            print(blacks)\n",
        "        return 1  # 성공 보상\n",
        "\n",
        "    if side == \"white\":\n",
        "        # print(piece, direction, side)\n",
        "        # print(\"before move white:\", whites)\n",
        "        whites[piece] = new_coord\n",
        "        grid[new_coord] = 1\n",
        "        # print(\"after move white:\", whites)\n",
        "    else:\n",
        "        blacks[piece] = new_coord\n",
        "        grid[new_coord] = 2\n",
        "\n",
        "    return 0  # 일반 보상"
      ],
      "metadata": {
        "id": "wJM4yimadNrX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 행동 디코딩 함수\n",
        "def decode_action(action, valid_pieces):\n",
        "    piece_idx = action // 4  # 말의 인덱스 (0부터 시작)\n",
        "    if piece_idx >= len(valid_pieces):\n",
        "        raise ValueError(\"Invalid action: Piece index out of range.\")\n",
        "    piece = valid_pieces[piece_idx]  # 유효한 말 중 선택\n",
        "    direction_idx = action % 4\n",
        "    directions = [\"left\", \"right\", \"up\", \"down\"]\n",
        "    return piece, directions[direction_idx]"
      ],
      "metadata": {
        "id": "Z4y7tZr6dPTQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DQN 학습 루프\n",
        "def train_dqn():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Q-Network 및 Target Network 초기화\n",
        "    policy_net = DQN(STATE_DIM, ACTION_DIM).to(device)\n",
        "    target_net = DQN(STATE_DIM, ACTION_DIM).to(device)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "    target_net.eval()\n",
        "\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
        "    memory = deque(maxlen=MEMORY_CAPACITY)\n",
        "\n",
        "    for episode in tqdm(range(1000), desc=\"Training Progress\"):\n",
        "        state = reset_environment()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        current_player = \"white\"  # 첫 번째 턴은 흰색 플레이어\n",
        "\n",
        "        while not done:\n",
        "            # Epsilon-Greedy 행동 선택\n",
        "            epsilon = max(0.1, 0.9 - 0.01 * episode)\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            if random.random() < epsilon:\n",
        "                action = random.randint(0, ACTION_DIM - 1)\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    action = policy_net(state_tensor).argmax(dim=1).item()\n",
        "\n",
        "            # 행동 수행 및 보상 계산\n",
        "            valid_pieces = list(whites.keys() if current_player == \"white\" else blacks.keys())\n",
        "            action = policy_net(state_tensor).argmax(dim=1).item()\n",
        "            piece, direction = decode_action(action, valid_pieces)\n",
        "            reward = move(piece, direction, current_player)  # 현재 플레이어가 행동\n",
        "            total_reward += reward\n",
        "\n",
        "            # 다음 상태 가져오기\n",
        "            next_state = grid.flatten()\n",
        "            done = len(whites) == 4 or len(blacks) == 4  # 모든 말이 골에 도달하면 종료\n",
        "\n",
        "            # 경험 리플레이 저장\n",
        "            memory.append((state, action, reward, next_state, done))\n",
        "            state = next_state\n",
        "\n",
        "            # 플레이어 교대\n",
        "            current_player = \"black\" if current_player == \"white\" else \"white\"\n",
        "\n",
        "            # 학습\n",
        "            if len(memory) >= BATCH_SIZE:\n",
        "                batch = random.sample(memory, BATCH_SIZE)\n",
        "                states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "                states = torch.tensor(states, dtype=torch.float32).to(device)\n",
        "                actions = torch.tensor(actions, dtype=torch.long).to(device).unsqueeze(1)\n",
        "                rewards = torch.tensor(rewards, dtype=torch.float32).to(device).unsqueeze(1)\n",
        "                next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
        "                dones = torch.tensor(dones, dtype=torch.float32).to(device).unsqueeze(1)\n",
        "\n",
        "                # Q-값 계산\n",
        "                q_values = policy_net(states).gather(1, actions)\n",
        "                next_q_values = target_net(next_states).max(dim=1, keepdim=True)[0]\n",
        "                target_q_values = rewards + GAMMA * next_q_values * (1 - dones)\n",
        "\n",
        "                loss = nn.MSELoss()(q_values, target_q_values)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        # 타겟 네트워크 업데이트\n",
        "        if episode % TARGET_UPDATE == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
        "\n",
        "    # 모델 저장\n",
        "    torch.save(policy_net.state_dict(), \"models/dqn_model.pth\")\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     train_dqn()\n",
        "train_dqn()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "collapsed": true,
        "id": "tjarfXE-dQVQ",
        "outputId": "89480d8c-0de8-4545-a35f-85b5b5c55cce"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Progress:   0%|          | 0/1000 [00:00<?, ?it/s]<ipython-input-7-ba4233f6a465>:53: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  states = torch.tensor(states, dtype=torch.float32).to(device)\n",
            "Training Progress:   0%|          | 0/1000 [24:24<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ba4233f6a465>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# if __name__ == \"__main__\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m#     train_dqn()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mtrain_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-ba4233f6a465>\u001b[0m in \u001b[0;36mtrain_dqn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dqn()"
      ],
      "metadata": {
        "id": "hK2g1YsIdTFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy_net"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "z92MygBxev_W",
        "outputId": "4da5fabc-04b1-45b2-d1d3-49de5cb8a61e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'policy_net' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-53c2b916af7e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'policy_net' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(policy_net.state_dict(), \"models/dqn_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "k-8Pwymtersm",
        "outputId": "852d22c6-3fef-4421-edcd-ff946751efb4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'policy_net' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-8e6bc5c056f2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"models/dqn_model.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'policy_net' is not defined"
          ]
        }
      ]
    }
  ]
}